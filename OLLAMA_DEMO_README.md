# Ollama Demo - Полный пример тегирования с тремя этапами

Этот notebook (`ollama_demo.ipynb`) демонстрирует полный цикл работы с библиотекой `llm_tags` для автоматического тегирования обращений клиентов с использованием локальной Ollama.

## Описание этапов

### Этап 1: Первичная обработка всех данных
- Загружаются все обращения из `demo_sample.csv.gz`
- Выполняется первичное тегирование всех обращений
- Создается базовый словарь тегов
- Результаты сохраняются в `results_stage1.csv` и `tags_dict_stage1.json`

### Этап 2: Дополнительный анализ не тегнутых обращений
- Анализируются обращения, которые не получили теги на этапе 1
- Используется более детальный промпт для сложных случаев
- Обновляется словарь тегов
- Результаты сохраняются в `results_stage2.csv` и `tags_dict_stage2.json`

### Этап 3: Финальная переразметка всех обращений
- Выполняется полная переразметка всех обращений с нуля
- Используется накопленный словарь тегов из этапов 1 и 2
- Создается консистентная финальная разметка
- Результаты сохраняются в `results_final.csv` и `tags_dict_final.json`

## Требования

### 1. Установка Ollama
```bash
# Установка Ollama (если еще не установлена)
curl -fsSL https://ollama.com/install.sh | sh

# Запуск Ollama сервера
ollama serve
```

### 2. Установка модели
```bash
# Установка модели qwen2.5:32b (рекомендуется)
ollama pull qwen2.5:32b

# Альтернативные модели (если qwen2.5:32b слишком большая):
# ollama pull qwen2.5:14b
# ollama pull qwen2.5:7b
# ollama pull llama3.1:8b
```

### 3. Python зависимости
```bash
# Активация виртуального окружения
source venv/bin/activate

# Установка зависимостей (если еще не установлены)
pip install pandas jupyter
```

## Запуск

### Способ 1: Jupyter Notebook
```bash
cd /home/alex/tradeML/llm_tags
source venv/bin/activate
jupyter notebook ollama_demo.ipynb
```

### Способ 2: JupyterLab
```bash
cd /home/alex/tradeML/llm_tags
source venv/bin/activate
jupyter lab ollama_demo.ipynb
```

### Способ 3: VS Code / Cursor
1. Откройте файл `ollama_demo.ipynb` в редакторе
2. Выберите Python kernel из виртуального окружения `venv`
3. Запускайте ячейки последовательно

## Настройки

### Изменение модели
В ячейке инициализации LLM можно изменить модель:

```python
llm = OllamaLLM(
    api_url="http://localhost:11434/api",
    model="qwen2.5:14b",  # Измените на другую модель
    temperature=0.7
)
```

### Настройка производительности
В ячейке инициализации Pipeline:

```python
pipeline = TaggingPipeline(
    llm=llm,
    batch_size=50,  # Уменьшите для медленных систем (20-30)
    num_workers=5   # Уменьшите для медленных систем (2-3)
)
```

### Рекомендации по производительности:
- **Быстрая система (GPU, много RAM)**: `batch_size=100`, `num_workers=10`
- **Средняя система**: `batch_size=50`, `num_workers=5` (по умолчанию)
- **Медленная система**: `batch_size=20`, `num_workers=2`

## Результаты

После выполнения всех ячеек будут созданы следующие файлы:

### Результаты этапов:
- `results_stage1.csv` - результаты первичной обработки
- `results_stage2.csv` - результаты после дополнительного анализа
- `results_final.csv` - финальные результаты

### Словари тегов:
- `tags_dict_stage1.json` - словарь тегов после этапа 1
- `tags_dict_stage2.json` - словарь тегов после этапа 2
- `tags_dict_final.json` - финальный словарь тегов

### Анализ:
- `comparison.csv` - сравнительная таблица всех этапов

## Структура данных

### Входные данные (`demo_sample.csv.gz`):
```
conversation_id, speaker, date_time, text, request_id
```

### Выходные данные (например, `results_final.csv`):
```
conversation_id, speaker, date_time, text, request_id, tags
```

Где `tags` - строка с тегами через запятую, например: `"тарифы, консультация"`

## Анализ результатов

Notebook включает подробный анализ:
- Статистика по количеству размеченных обращений
- Топ-20 самых популярных тегов
- Распределение по количеству тегов на обращение
- Анализ по типу speaker (agent/client)
- Сравнение результатов всех трех этапов
- Примеры размеченных обращений

## Время выполнения

Примерное время для обработки 1000 обращений:
- **Этап 1**: 10-20 минут (зависит от модели и железа)
- **Этап 2**: 2-5 минут (обрабатываются только не тегнутые)
- **Этап 3**: 10-20 минут (полная переразметка)

**Общее время**: 20-45 минут для 1000 обращений

## Troubleshooting

### Ошибка: "Не удалось подключиться к Ollama"
```bash
# Проверьте что Ollama запущена
ps aux | grep ollama

# Если не запущена, запустите:
ollama serve
```

### Ошибка: "Model not found"
```bash
# Установите модель
ollama pull qwen2.5:32b

# Проверьте установленные модели
ollama list
```

### Медленная работа
- Уменьшите `batch_size` и `num_workers`
- Используйте более легкую модель (qwen2.5:7b вместо qwen2.5:32b)
- Убедитесь что используется GPU (если доступен)

### Out of Memory
- Уменьшите `batch_size` до 10-20
- Уменьшите `num_workers` до 1-2
- Используйте более легкую модель

## Дополнительные возможности

### Изменение промптов
Вы можете настроить промпты для каждого этапа:
- `tag_prompt` - для этапа 1
- `detailed_prompt` - для этапа 2
- `final_prompt` - для этапа 3

### Работа с собственными данными
Замените путь к файлу в ячейке загрузки данных:
```python
data_path = "/путь/к/вашим/данным.csv"
df = pd.read_csv(data_path)
```

Убедитесь что в данных есть колонка `text` с текстом обращений.

### Экспорт результатов
Результаты автоматически сохраняются в CSV и JSON форматах.
Вы можете дополнительно экспортировать в другие форматы:

```python
# Экспорт в Excel
result_df_final.to_excel("results_final.xlsx", index=False)

# Экспорт в Parquet
result_df_final.to_parquet("results_final.parquet", index=False)
```

## Контакты и поддержка

Если у вас возникли вопросы или проблемы:
1. Проверьте что Ollama запущена и модель установлена
2. Проверьте логи в notebook
3. Обратитесь к документации библиотеки `llm_tags`

## Лицензия

Этот пример является частью проекта `llm_tags`.


# Changelog

## Изменения в промптах (последнее обновление)

### Что изменилось:

1. **Автоматическое добавление существующих тегов в промпт**
   - Теперь найденные ранее теги автоматически подставляются в промпт для LLM
   - Это позволяет LLM переиспользовать существующие теги и поддерживать консистентность

2. **Упрощенный базовый промпт**
   - Базовый промпт теперь содержит только то, что передается в параметр `tag_prompt` функции `tag()`
   - Убраны избыточные инструкции из базового промпта
   - Технические инструкции (формат JSON, правила) добавляются автоматически

### Структура финального промпта:

```
[Пользовательский промпт]

[Существующие теги - если есть]

Обращения для тегирования:
[Список обращений]

[Инструкции по формату JSON]

[Правила тегирования]
```

### Пример использования:

```python
from llm_tags import TaggingPipeline, OllamaLLM

llm = OllamaLLM()
pipeline = TaggingPipeline(llm=llm)

# Простой промпт - только ваши инструкции
prompt = """Проанализируй обращения клиентов и определи теги.
Теги должны отражать основную тему обращения."""

# Первый батч - без существующих тегов
result_df1, tags_dict1 = pipeline.tag(
    df1,
    text_column="text",
    tag_prompt=prompt,
    max_tags=2
)

# Второй батч - существующие теги автоматически добавятся в промпт
result_df2, tags_dict2 = pipeline.tag(
    df2,
    text_column="text",
    tag_prompt=prompt,
    existing_tags=tags_dict1,  # Передаем найденные ранее теги
    max_tags=2
)
```

### Преимущества:

1. **Консистентность**: LLM видит ранее созданные теги и может их переиспользовать
2. **Простота**: Пользователь передает только свой промпт, без технических деталей
3. **Гибкость**: Можно легко изменить пользовательский промпт, не трогая техническую часть
4. **Масштабируемость**: При обработке больших датасетов теги накапливаются и переиспользуются

### Демонстрационные скрипты:

- `show_prompt_example.py` - показывает как формируется финальный промпт
- `test_new_prompt.py` - тестирует работу с существующими тегами


# Сводка изменений

## Что было сделано

### 1. Изменена логика формирования промпта

**Файл:** `llm_tags.py`

**Изменения в методе `tag_batch` (для обоих классов `OllamaLLM` и `OpenAICompatibleLLM`):**

#### Было (старая версия):
```python
full_prompt = f"""{prompt}

{existing_tags_text}

Обращения для тегирования:
{requests_text}

ВАЖНО: Верни ТОЛЬКО валидный JSON массив (начинается с [ и заканчивается ]), где для каждого обращения указаны теги и их описания.

Формат ответа (ОБЯЗАТЕЛЬНО массив):
[...]

Правила:
- Максимум {max_tags} тегов на обращение
- ...
"""
```

#### Стало (новая версия):
```python
full_prompt = f"""{prompt}{existing_tags_text}

Обращения для тегирования:
{requests_text}

Верни ТОЛЬКО валидный JSON массив в формате:
[...]

Правила:
- Максимум {max_tags} тегов на обращение
- ...
"""
```

### 2. Ключевые отличия:

1. **Убрано слово "ВАЖНО:"** - промпт стал более лаконичным
2. **Упрощена формулировка** - "Верни ТОЛЬКО валидный JSON массив в формате:" вместо "ВАЖНО: Верни ТОЛЬКО валидный JSON массив (начинается с [ и заканчивается ]), где для каждого обращения указаны теги и их описания."
3. **Убрана строка "Формат ответа (ОБЯЗАТЕЛЬНО массив):"** - избыточное уточнение
4. **Существующие теги автоматически добавляются** - если передан параметр `existing_tags`, они включаются в промпт

### 3. Как это работает:

```python
# Пользователь передает только свой промпт
user_prompt = "Проанализируй обращения клиентов и определи теги."

# Система автоматически:
# 1. Добавляет существующие теги (если есть)
# 2. Добавляет обращения для тегирования
# 3. Добавляет инструкции по формату JSON
# 4. Добавляет правила тегирования

# Финальный промпт отправляется в LLM
```

### 4. Преимущества:

✅ **Простота** - пользователь передает только свой промпт  
✅ **Консистентность** - существующие теги автоматически переиспользуются  
✅ **Гибкость** - легко изменить пользовательский промпт  
✅ **Чистота** - нет избыточных инструкций в базовом промпте  

### 5. Демонстрационные файлы:

- `show_prompt_example.py` - показывает структуру финального промпта
- `test_new_prompt.py` - тестирует работу с существующими тегами
- `CHANGELOG.md` - подробное описание изменений

### 6. Обратная совместимость:

✅ Все существующие скрипты продолжат работать  
✅ API не изменился - те же параметры функции `tag()`  
✅ Формат ответа остался прежним  

## Как использовать:

```python
from llm_tags import TaggingPipeline, OllamaLLM

llm = OllamaLLM()
pipeline = TaggingPipeline(llm=llm)

# Ваш простой промпт
prompt = "Проанализируй обращения и определи теги."

# Первый батч
result1, tags1 = pipeline.tag(df1, text_column="text", tag_prompt=prompt)

# Второй батч - теги из первого батча автоматически добавятся
result2, tags2 = pipeline.tag(df2, text_column="text", tag_prompt=prompt, existing_tags=tags1)
```

---

**Дата изменений:** 30 ноября 2025  
**Версия:** 1.1

